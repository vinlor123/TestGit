{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"!pip install tensorflow-gpu\n!pip install --upgrade grpcio\n!pip install bert-for-tf2\n!pip install sentencepiece","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint (tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport math\nimport datetime\n\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib import rc\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nRANDOM_SEED = 42\n\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all2 = pd.read_csv('../input/training2110/Dummy_public_train_73.csv')\ndata_all2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = data_all2\nprint (len(data_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (data_all['class'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all3 = pd.read_csv('../input/training2110/Dummy_public_test_73.csv')\ndata_all3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all3_topic_only = data_all3[data_all3['class'] != 0]\ndata_all_topic_only = data_all[data_all['class'] != 0]\nprint (data_all_topic_only['class'].unique())\nprint (data_all3_topic_only['class'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data_all_topic_only\ntest = data_all3_topic_only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#train, test = train_test_split(data_all, test_size=0.2, random_state=42)\n#print (len(train))\n#print (len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape\nprint (train['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\nfolder = 'model_folder'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model_name=\"uncased_L-12_H-768_A-12\"\nBERT_PRETRAINED_DIR = f'{folder}/'\nbert_ckpt_dir = os.path.join(BERT_PRETRAINED_DIR, bert_model_name)\nbert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PrepareData:\n  DATA_COLUMN = \"sentence\"\n  LABEL_COLUMN = \"class\"\n\n  def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n    self.tokenizer = tokenizer\n    self.max_seq_len = 250\n    self.classes = classes\n    \n    ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n\n    print(\"max seq_len\", self.max_seq_len)\n    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n#     self.max_seq_len = max(self.max_seq_len, max_seq_len)\n    print(\"max seq_len\", self.max_seq_len)\n    self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n\n  def _prepare(self, df):\n    x, y = [], []\n    \n    for _, row in tqdm(df.iterrows()):\n      text, label = row[PrepareData.DATA_COLUMN], row[PrepareData.LABEL_COLUMN]\n      tokens = self.tokenizer.tokenize(str(text))\n      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n      x.append(token_ids)\n      y.append(self.classes.index(label))\n\n    return np.array(x), np.array(y)\n\n  def _pad(self, ids):\n    x = []\n    for input_ids in ids:\n      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n      x.append(np.array(input_ids))\n    return np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\ntokens = tokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")\ntokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(max_seq_len, bert_ckpt_file):\n  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n      bc = StockBertConfig.from_json_string(reader.read())\n      bert_params = map_stock_config_to_params(bc)\n      bert_params.adapter_size = None\n      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n        \n  input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n  bert_output = bert(input_ids)\n  print(\"bert shape\", bert_output.shape)\n  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n  cls_out = keras.layers.Dropout(0.2)(cls_out)\n  logits = keras.layers.Dense(units=128, activation=\"relu\")(cls_out)\n  logits = keras.layers.Dropout(0.2)(logits)\n  logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n  model = keras.Model(inputs=input_ids, outputs=logits)\n  model.build(input_shape=(None, max_seq_len))\n  load_stock_weights(bert, bert_ckpt_file)\n        \n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = sorted(train['class'].unique().tolist())\n\ndata = PrepareData(train, test, tokenizer, classes, max_seq_len=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.get_dummies(train['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1 = train['class']\ny1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\ny_integers = y1\nclass_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\nd_class_weights = dict(enumerate(class_weights))\nd_class_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(data.max_seq_len, bert_ckpt_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n#optimizer=keras.optimizers.Adam(1e-5),\noptimizer=keras.optimizers.Adam(1e-5),\nloss=keras.losses.CategoricalCrossentropy(from_logits=True),\nmetrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_dir = \"log/intent_detection/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n\nepochs=50\nhistory = model.fit(x=data.train_x,\n                    y=y,\n                    validation_split=0.2,\n                    batch_size=16,\n                    epochs=epochs,\n                    #callbacks=callbacks_list,\n                    class_weight = d_class_weights,\n                    verbose=1,\n                    shuffle=True)\n#                     validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_test = pd.get_dummies(data.test_y)\ny_test = pd.get_dummies(test['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, train_acc = model.evaluate(data.train_x,y)\n_, test_acc = model.evaluate(data.test_x,y_test)\n\nprint(\"train acc\", train_acc)\nprint(\"test acc\", test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(data.test_x).argmax(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_class_name = [\n    'Net Sales',\n    'Operating Profit Margin',\n    'Risk',\n    'Non Current Assets',\n    'Total Operating Expenses',\n    'Net Worth',\n    'Total Debt',\n    'NPAT Margin',\n    'Cashflow',\n    'Current Asset',\n    'Operating Profit',\n    'Contingent Liabilities/Guarantee',\n    'Current Liabilities',\n    'Cash and Equivalents',\n    'EBITDA Margin',\n    'GP margin',\n    'Non Current Liabilities'\n    ]\ntarget_class_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(np.argmax(y_test.values, axis = 1),y_pred, target_names = target_class_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"DNN_model_2010.h5\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}